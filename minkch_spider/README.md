# minkch.com çˆ¬è™«

## ä¸€ åŠŸèƒ½

è‡ªåŠ¨çˆ¬å– minkch.com çš„æ‰€æœ‰å›¾ç‰‡è§†é¢‘ï¼Œå¹¶åˆ†ç±»ä¿å­˜å„æ•°æ®é¡¹åˆ°æ•°æ®åº“ã€‚åœ¨ `settings.py` è¿›è¡Œå¯é…ç½®çš„çˆ¬å–ã€‚

## äºŒ å‡†å¤‡

### 1. è™šæ‹Ÿç¯å¢ƒå‡†å¤‡

å®‰è£…ï¼š
- è™šæ‹Ÿç¯å¢ƒï¼š`pip install virtualenv`
- è™šæ‹Ÿç¯å¢ƒç®¡ç†å™¨ï¼š`pip install virtualenvwrapper-win`

ä¸€äº›åŸºæœ¬çš„å‘½ä»¤ï¼š
- æ›´æ”¹WORKON_HOMEï¼šæ‰“å¼€æ§åˆ¶é¢æ¿-ç³»ç»Ÿå’Œå®‰å…¨-ç³»ç»Ÿ-é«˜çº§ç³»ç»Ÿè®¾ç½®-ç¯å¢ƒå˜é‡-ç³»ç»Ÿå˜é‡-ç‚¹å‡»æ–°å»ºï¼Œå˜é‡åï¼šè¾“å…¥ `WORKON_HOME`ï¼Œ å˜é‡å€¼ï¼šè¾“å…¥è‡ªå®šä¹‰çš„è·¯å¾„ï¼Œç¡®å®šä¿å­˜ï¼Œæœ€å**é‡å¯**æˆ–è€…**åˆ©ç”¨ win+r è¿›å…¥ cmd.exe**å³å¯æˆåŠŸæ›´æ”¹ç›®å½•
- `mkvirtualenv spider`ï¼šåˆ›å»ºè™šæ‹Ÿç¯å¢ƒspider
- `workon spider`ï¼šè¿›å…¥è™šæ‹Ÿç¯å¢ƒspider
- `deactivate`ï¼šé€€å‡ºè™šæ‹Ÿç¯å¢ƒ
- `rmvirtualenv spider`ï¼šç§»é™¤è™šæ‹Ÿç¯å¢ƒspider

### 2. ä¾èµ–åŒ…å‡†å¤‡

> - å›½å†…é•œåƒï¼š`pip install -i https://pypi.douban.com/simple python_package_name`
> - ç¬¬ä¸‰æ–¹ whl æ–‡ä»¶ä¸‹è½½æä¾›ï¼š[Unofficial Windows Binaries for Python Extension Packages](https://www.lfd.uci.edu/~gohlke/pythonlibs/)

- `pip install lxml` HTML å’Œ XML çš„è§£æåº“
- `pip install twisted` å¼‚æ­¥æ¡†æ¶
- `pip install pywin32` Windows API åº“
- `pip install scrapy` Scrapy æ¡†æ¶
- `pip install pillow` Scrapy å›¾ç‰‡ä¸‹è½½ç”¨åˆ°çš„å›¾ç‰‡å¤„ç†åº“
- `pip install mysqlclient` MySQL æ•°æ®åº“è¿æ¥åº“

### 3. æ•°æ®åº“å‡†å¤‡

```
mysql> desc archives;
+------------------------+-----------+------+-----+---------+-------+
| Field                  | Type      | Null | Key | Default | Extra |
+------------------------+-----------+------+-----+---------+-------+
| id                     | int       | NO   | PRI | NULL    |       |
| title                  | char(100) | NO   |     | NULL    |       |
| title_zh               | char(100) | YES  |     | NULL    |       |
| date_time              | datetime  | YES  |     | NULL    |       |
| tags                   | char(100) | YES  |     | NULL    |       |
| comments               | int       | YES  |     | NULL    |       |
| pre_url                | char(100) | YES  |     | NULL    |       |
| next_url               | char(100) | YES  |     | NULL    |       |
| img_scalar             | tinyint   | YES  |     | NULL    |       |
| video_scalar           | tinyint   | YES  |     | NULL    |       |
| img_got                | tinyint   | YES  |     | NULL    |       |
| video_got              | tinyint   | YES  |     | NULL    |       |
| img_acquisition_rate   | float     | YES  |     | NULL    |       |
| video_acquisition_rate | float     | YES  |     | NULL    |       |
| src_path               | text      | NO   |     | NULL    |       |
+------------------------+-----------+------+-----+---------+-------+
```

### 4. Scrapy å…·ä½“ä½¿ç”¨

1. åœ¨`WORKON_HOME`çš„ç›®å½•ä¸‹ï¼Œä½¿ç”¨`mkvirtualenv env_for_minkch_spider`
2. `workon env_for_minkch_spider`
3. `scrapy startproject MinkchSpider`
4. `cd MinkchSpider`
5. `scrapy genspider minkch minkch.com`
6. ç¼–è¾‘ä»¥ä¸‹æºä»£ç å³å¯ğŸ‘‡

## ä¸‰ æºç 

> PS. åªå±•ç¤ºä¸Scrapyæ¡†æ¶ä¸åŒçš„éƒ¨åˆ†

### `MinkchSpider\main.py`

```python
from scrapy.cmdline import execute

import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

execute(["execute", "crawl", "minkch"])

```

### `MinkchSpider\MinkchSpider\items.py`

```python
import re

from scrapy.loader import ItemLoader
from scrapy.item import Item, Field
from scrapy.loader.processors import MapCompose, TakeFirst, Identity, Join


def id_convert(value):
    match = re.match(r".*/(\d+).html", value)
    if match:
        return match.group(1)
    else:
        print('[My Log] idè·å–å¤±è´¥ï¼Œåœ¨.\\MinkchSpider\\items.py')


def comments_convert(value):
    match = re.match(r".*?(\d+).*", value)
    if match:
        return match.group(1)
    else:
        print('[My Log] commentsè·å–å¤±è´¥ï¼Œåœ¨.\\MinkchSpider\\items.py')


def fill_link(value):
    return 'https:' + value


class img_scalar_convert:
    def __call__(self, values):
        for value in values:
            match = re.match(r".*?(\d+)æš", value)
            if match:
                return match.group(1)


class video_scalar_convert:
    def __call__(self, values):
        for value in values:
            match = re.match(r".*?(\d+)æœ¬", value)
            if match:
                return match.group(1)


class MinkchItemLoader(ItemLoader):
    default_output_processor = TakeFirst()


class MinkchItem(Item):
    id = Field(
        input_processor=MapCompose(id_convert)
    )
    title = Field()
    title_zh = Field()
    date_time = Field()
    tags = Field(
        output_processor=Join(separator=',')
    )
    comments = Field(
        input_processor=MapCompose(comments_convert)
    )
    pre_url = Field()
    next_url = Field()
    img_scalar = Field(
        output_processor=img_scalar_convert()
    )
    video_scalar = Field(
        output_processor=video_scalar_convert()
    )
    img_urls = Field(
        input_processor=MapCompose(fill_link),
        output_processor=Identity()
    )
    video_urls = Field(
        input_processor=MapCompose(fill_link),
        output_processor=Identity()
    )
    media_urls = Field()
    img_acquisition_rate = Field()
    video_acquisition_rate = Field()
    src_path = Field()

```

### `MinkchSpider\MinkchSpider\pipelines.py`

```python
import re

from scrapy.pipelines.files import FilesPipeline
from twisted.enterprise import adbapi
from MySQLdb.cursors import DictCursor


class MediaPpl(FilesPipeline):
    def file_path(self, request, response=None, info=None):
        # æ ¹æ®urlè®¾ç½®è·¯å¾„ eg. 2020\08\05063857\15965773010.jpg
        match = re.match(r'.*/(\d+/\d+.+)', request.url)
        if match:
            path = match.group(1)[:4] + '/' + match.group(1)[4:6] + '/' + match.group(1)[6:]
            return path
            # scrapy é»˜è®¤ä½¿ç”¨â€˜/â€™
        else:
            match = re.match(r'https://.*com/(.*)', request.url)
            if match:
                return match.group(1)
            else:
                print('[My Log] filepathå®šä¹‰å¤±è´¥ï¼Œåœ¨.\\MinkchSpider\\pipelines.py')


class MysqlPpl(object):
    def __init__(self, dbpool):
        self.dbpool = dbpool

    @classmethod
    def from_settings(cls, settings):
        dbparms = dict(
            host=settings['MYSQL_HOST'],
            db=settings['MYSQL_DBNAME'],
            user=settings['MYSQL_USER'],
            passwd=settings['MYSQL_PASSWORD'],
            charset='utf8',
            cursorclass=DictCursor,
            use_unicode=True
        )
        dbpool = adbapi.ConnectionPool('MySQLdb', **dbparms)
        return cls(dbpool)

    def process_item(self, item, spider):
        query = self.dbpool.runInteraction(self.do_insert, item)
        query.addErrback(self.handle_error, item, spider)

    def handle_error(self, failure, item, spider):
        print(failure)

    def do_insert(self, cursor, item):
        try:
            insert_sql = """
            replace into archives
            values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            params = list()
            params.append(item.get('id', 0))
            params.append(item.get('title', ''))
            params.append(item.get('title_zh', ''))
            params.append(item.get('date_time', '1601-01-02 23:59:59'))
            params.append(item.get('tags', ''))
            params.append(item.get('comments', 0))
            params.append(item.get('pre_url', ''))
            params.append(item.get('next_url', ''))
            params.append(item.get('img_scalar', 0))
            params.append(item.get('video_scalar', 0))
            params.append(len(item.get('img_urls', [])))
            params.append(len(item.get('video_urls', [])))
            params.append(item.get('img_acquisition_rate', 0))
            params.append(item.get('video_acquisition_rate', 0))
            params.append(item.get('src_path', ''))
            cursor.execute(insert_sql, tuple(params))
            print('[My Log] Save the data into MySQL Databases: Succeed')
        except:
            print('[My Log] Save the data into MySQL Databases: Fail')

```

### `MinkchSpider\MinkchSpider\settings.py`

```python
import os

BOT_NAME = 'MinkchSpider'

SPIDER_MODULES = ['MinkchSpider.spiders']
NEWSPIDER_MODULE = 'MinkchSpider.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure item pipelines
ITEM_PIPELINES = {
   'MinkchSpider.pipelines.MediaPpl': 1,
   'MinkchSpider.pipelines.MysqlPpl': 2,
}

# çˆ¬è™«çˆ¬å–èŒƒå›´
page_range = (200, 250)  # çˆ¬å–åˆ—è¡¨é¡µé¡µç èŒƒå›´ï¼Œç«¯ç‚¹å€¼éƒ½åŒ…æ‹¬
START_PAGE = page_range[0]  # çˆ¬å–çš„archiveåˆ—è¡¨é¡µçš„èµ·å§‹é¡µç ï¼Œæ¯ä¸€ä¸ªåˆ—è¡¨é¡µåŒ…å«11ç¯‡æ–‡ç« ï¼š1ä¸ªå¹¿å‘Šï¼Œ10ä¸ªarchives
ARCHIVE_CRAWL_LIMIT = 10 * (page_range[1]-page_range[0]+1)  # çˆ¬å–archiveçš„ä¸Šé™

# å›¾ç‰‡å’Œè§†é¢‘çš„ä¸‹è½½å¼€å…³
DL_IMG = True
DL_VIDEO = True

# ç½‘ç«™åŸºç¡€å‚æ•°ï¼Œå³archive per pageçš„æ•°é‡
APP = 10

# ç™¾åº¦ç¿»è¯‘APIçš„IDå’Œå¯†é’¥
BAIDU_APPID = '20200805000533496'
BAIDU_SECRETKEY = 'GNDsrRwMujz3MpFkgwlE'

# ä½¿ç”¨scrapy item pipelinesè¦é…ç½®çš„å‚æ•°
FILES_URLS_FIELD = 'media_urls'
project_dir = os.path.dirname(os.path.abspath(__file__))
FILES_STORE = os.path.join(project_dir, 'medias')

# æ•°æ®åº“ç›¸å…³å‚æ•°
MYSQL_HOST = 'localhost'
MYSQL_DBNAME = 'minkch_spider'
MYSQL_USER = 'root'
MYSQL_PASSWORD = 'root'

```

### `MinkchSpider\MinkchSpider\spiders\minkch.py`

```python
import json
import re

import scrapy
from scrapy import Request

from MinkchSpider.items import MinkchItemLoader
from MinkchSpider.items import MinkchItem
from MinkchSpider.utils import common
from MinkchSpider import settings


class MinkchSpider(scrapy.Spider):
    name = 'minkch'
    allowed_domains = ['minkch.com']
    start_urls = ['https://minkch.com/page/{}'.format(settings.START_PAGE)]

    archive_crawl_count = 0  # archiveçˆ¬å–æ•°é‡è®¡æ•°å™¨

    # çˆ¬å–start_urlï¼Œä¹Ÿå³åˆ—è¡¨å¼€å§‹é¡µçš„ç¬¬ä¸€æ¡archiveçš„é“¾æ¥å¹¶yieldç»™parse_archives
    def parse(self, response):
        first_archive_url = response.xpath('//a[@rel="bookmark" and not(@target="_blank")][1]/@href').extract_first('')
        yield Request(url=first_archive_url, callback=self.parse_archives)

    # çˆ¬å–è¯¦æƒ…é¡µï¼ˆarchiveé¡µï¼‰
    def parse_archives(self, response):

        # è®¾ç½®è®¡æ•°å™¨ï¼Œé€šè¿‡settings.pyä¸­çš„ARCHIVE_CRAWL_LIMITé¡¹æ¥ç¡®å®šçˆ¬å–archivesçš„ä¸Šé™
        MinkchSpider.archive_crawl_count += 1

        # æ—¥å¿—ï¼šæ‰“å°å½“å‰é¡µç å’Œå½“å‰archive
        current_page = settings.START_PAGE + MinkchSpider.archive_crawl_count // settings.APP
        current_archive = MinkchSpider.archive_crawl_count % settings.APP
        print('[My Log] Crawling page {}, archive {}'.format(current_page, current_archive))

        # çˆ¬å–å›¾ç‰‡çš„ä¸¤ç§xpathï¼Œåˆ†åˆ«å¯ä»¥è§£ææºå›¾ç‰‡å’Œç½‘é¡µä¸Šæ˜¾ç¤ºçš„å›¾ç‰‡
        img_xpath = {
            'original': '//div[@class="entry-content clearfix"][last()]//img[@class="pict"]/parent::a/attribute::href',
            'displaying': '//div[@class="entry-content clearfix"][last()]//img[@class="pict"]/attribute::src'}

        # é€šè¿‡è‡ªå®šä¹‰çš„ItemLoaderæ¥è§£æå„é¡¹æ•°æ®åˆ°item
        item_loader = MinkchItemLoader(item=MinkchItem(), response=response)
        item_loader.add_value('id', response.url)
        item_loader.add_xpath('title', '//h2[@class="h2 entry-title "]/span/text()')
        item_loader.add_xpath('date_time', '//*[@class="entry-meta-default"]//time/attribute::datetime')
        item_loader.add_xpath('tags', '//div[@class="entry-utility entry-meta"]/a/text()')
        item_loader.add_xpath('comments', '//*[@class="entry-meta-default"]//em/text()')
        item_loader.add_xpath('pre_url', '//*[@id="nav-below"]//a[@rel="prev"]/attribute::href')
        item_loader.add_xpath('next_url', '//*[@id="nav-below"]//a[@rel="next"]/attribute::href')
        item_loader.add_xpath('img_scalar', '//div[@class="entry-content clearfix"][last()]//strong/text()')
        item_loader.add_xpath('video_scalar', '//div[@class="entry-content clearfix"][last()]//strong/text()')
        item_loader.add_xpath('img_urls', img_xpath['original'])
        item_loader.add_xpath('video_urls', '//video//source/attribute::src')
        archive_item = item_loader.load_item()

        # ä½¿ç”¨ç™¾åº¦ç¿»è¯‘api
        yield Request(url=common.get_trans_url(archive_item['title']),
                      meta={'archive_item': archive_item},
                      callback=self.parse_rest,
                      dont_filter=True)

        # è®¡æ•°æ¯”è¾ƒ
        if MinkchSpider.archive_crawl_count < settings.ARCHIVE_CRAWL_LIMIT:
            yield Request(url=archive_item['pre_url'], callback=self.parse_archives)

    # çˆ¬å–å‰©ä½™çš„ï¼šâ‘ ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜ â‘¡åª’ä½“èµ„æºæ±‡æ€» â‘¢èµ„æºè·å–ç‡ â‘£èµ„æºæ‰€åœ¨æ–‡ä»¶å¤¹ç›¸å¯¹è·¯å¾„ï¼›å¹¶ä¸”æœ€ç»ˆyield item
    def parse_rest(self, response):

        # ä¸­æ–‡ç¿»è¯‘çš„æ ‡é¢˜
        archive_item = response.meta.get('archive_item', '')
        json_data = json.loads(response.text)
        if 'trans_result' in json_data:
            archive_item['title_zh'] = json_data['trans_result'][0]['dst']
        else:
            print('[My Log] æœªæˆåŠŸè·å¾—ç¿»è¯‘ç»“æœï¼Œåœ¨.\\MinkchSpider\\minkch.py')

        # åª’ä½“èµ„æºæ±‡æ€»
        archive_item['media_urls'] = []
        if settings.DL_IMG:
            archive_item['media_urls'] += archive_item.get('img_urls', [])
        if settings.DL_VIDEO:
            archive_item['media_urls'] += archive_item.get('video_urls', [])

        # èµ„æºè·å–ç‡
        if int(archive_item.get('img_scalar', 0)) != 0:
            archive_item['img_acquisition_rate'] = \
                len(archive_item.get('img_urls', [])) / int(archive_item.get('img_scalar', 0))
        else:
            archive_item['img_acquisition_rate'] = 1.0
        if int(archive_item.get('video_scalar', 0)) != 0:
            archive_item['video_acquisition_rate'] = \
                len(archive_item.get('video_urls', [])) / int(archive_item.get('video_scalar', 0))
        else:
            archive_item['video_acquisition_rate'] = 1.0

        # èµ„æºæ‰€åœ¨æ–‡ä»¶å¤¹ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹åœ¨MinkchSpider\medias\ä¸‹ï¼Œå®é™…å³ä¸ºæ–‡ä»¶å¤¹åç§°
        # èµ„æºè·¯å¾„ä»…ä½œå‚è€ƒï¼Œç”±äºç½‘ç«™è®¾è®¡åŸå› ï¼Œå¹¶ä¸å‡†ç¡®
        archive_item['src_path'] = ''
        for media_url in archive_item.get('media_urls', ['']):
            match = re.match(r'.*/(\d+)/\d+.+', media_url)
            if match:
                archive_item['src_path'] = \
                    match.group(1)[:4] + '\\' + match.group(1)[4:6] + '\\' + match.group(1)[6:] + '\\'
                break
        if archive_item['src_path'] == '':
            for media_url in archive_item.get('media_urls', ['']):
                match = re.match(r'https://.*com/(.*)', media_url)
                if match:
                    archive_item['src_path'] += match.group(1) + ', '

        yield archive_item

```

### `MinkchSpider\MinkchSpider\utils\common.py`

```python
# common.py contains common functions for minkch project

import hashlib
import urllib.parse
import random

from MinkchSpider import settings


def get_trans_url(q='apple'):
    appid = settings.BAIDU_APPID  # å¡«å†™ä½ çš„appid
    secret_key = settings.BAIDU_SECRETKEY  # å¡«å†™ä½ çš„å¯†é’¥

    my_url = 'https://api.fanyi.baidu.com/api/trans/vip/translate'

    from_lang = 'auto'  # åŸæ–‡è¯­ç§
    to_lang = 'zh'  # è¯‘æ–‡è¯­ç§
    salt = random.randint(32768, 65536)
    sign = appid + q + str(salt) + secret_key
    sign = hashlib.md5(sign.encode()).hexdigest()
    my_url = my_url + '?appid=' + appid + '&q=' + urllib.parse.quote(q) + '&from=' + from_lang + '&to=' + to_lang + \
        '&salt=' + str(salt) + '&sign=' + sign
    return my_url

```